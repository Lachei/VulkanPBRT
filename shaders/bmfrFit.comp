#version 460

#include "bmfrGeneral.comp" //includes all layout declarations and common functions

shared vec3 ws[ALPHA_SIZE - 3];
shared float uLengthSquared;
void main(){
    //note: this kernel always runs in a one dimensional block, so local_size_x,y = 1
    const int groupId = int(gl_WorkGroupID.x * gl_NumWorkGroups.y + gl_WorkGroupID.y);
    const int id = int(gl_LocalInvocationIndex);
    const ivec2 basePixel = ivec2(gl_WorkGroupID.xy) * ivec2(PIXEL_BLOCK_WIDTH, PIXEL_BLOCK_WIDTH);


    //load features and add noise
    float features[PIXEL_BLOCK / BLOCK_WIDTH][ALPHA_SIZE];
    for(int col = 0; col < int(ALPHA_SIZE); ++col){
        for(int subVector = 0; subVector < PIXEL_BLOCK / BLOCK_WIDTH; ++subVector){
            const int index = id + subVector * BLOCK_WIDTH;     //holds the linearized pixel index in the block
            const ivec2 pixelCoords = ivec2(index / PIXEL_BLOCK_WIDTH, index % PIXEL_BLOCK_WIDTH) + basePixel;
            float tmp = imageLoad(featureBuffer, ivec3(pixelCoords, col)).x;
            if(col < ALPHA_SIZE - 3) tmp = addRandom(tmp, id, subVector, col, int(camParams.frameNumber));
            features[subVector][col] = tmp;
        }
    }

    //compute r
    for(int col = 0; col < int(ALPHA_SIZE - 3); ++col){
        float u[PIXEL_BLOCK / BLOCK_WIDTH];
        float val2 = 0; //corresponds to tmpSum in standard BMFR implementation
        for(int subVector = 0; subVector < PIXEL_BLOCK / BLOCK_WIDTH; ++subVector){
            const int index = id + subVector * BLOCK_WIDTH;
            u[subVector] = features[subVector][col];
            if(index > col) val2 += u[subVector] * u[subVector];
        }

        float vecLenSqu = parallel_reduction_sum(val2);    // corresponds to vecLength (is the length squared)
        float vecLen = 0;
        if(id < col)
            u[0] = 0;
        else if(id == col){
            vecLen = sqrt(vecLenSqu + u[0] * u[0]);
            u[0] -= vecLen;
            vecLenSqu += u[0] * u[0];
            uLengthSquared = vecLenSqu;
            features[0][col] = vecLen;
        }
        else{
            features[0][col] = 0;
        }
        barrier();          //synchronizing so that every thread has uLenghtSquared available

        //transformation of all other columns in the feature matrix
        for(int f = col + 1; f < int(ALPHA_SIZE); ++f){
            float v = 0; // tmpSum in original implementaion
            for(int subVector = 0; subVector < PIXEL_BLOCK / BLOCK_WIDTH; ++subVector){
                const int index = id + subVector * BLOCK_WIDTH;
                if(index >= col){
                    v += features[subVector][f] * u[subVector];
                }
            }
            v = parallel_reduction_sum(v);
            for(int subVector = 0; subVector < PIXEL_BLOCK / BLOCK_WIDTH; ++subVector){
                const int index = id + subVector * BLOCK_WIDTH;
                if(index >= col){
                    features[subVector][f] -= 2 * u[subVector] * v / uLengthSquared;
                }
            }
        }
    }

    //back substitution
    for(int i = int(ALPHA_SIZE - 4); i >= 0; --i){
        if(id == i){
            ws[i] = vec3(features[0][ALPHA_SIZE - 3], features[0][ALPHA_SIZE - 2], features[0][ALPHA_SIZE - 1]);
            for(int x = i + 1; x < int(ALPHA_SIZE - 3); ++x){
                ws[i] -= ws[x] * features[0][x];
            }
            ws[i] /= features[0][i];
        }
        barrier();
    }

    // store weights
    if(id < ALPHA_SIZE - 3){
        vec3 weight = ws[id];
        if(uLengthSquared == 0) weight = vec3(.2);
        
        imageStore(weights, ivec3(gl_WorkGroupID.xy, id * 3), vec4(weight.x));
        imageStore(weights, ivec3(gl_WorkGroupID.xy, id * 3 + 1), vec4(weight.y));
        imageStore(weights, ivec3(gl_WorkGroupID.xy, id * 3 + 2), vec4(weight.z));
    }
}